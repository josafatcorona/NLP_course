{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f03ba61",
   "metadata": {},
   "source": [
    "# Introduction to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8528302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1bb84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082fe7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e796067",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris.target #labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece47656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class 0 --> [1,0,0]\n",
    "#class 1 --> [0,1,0]\n",
    "#class 2 --> [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b55ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras\n",
    "# !pip install tensorflow\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7664d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeded52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train ant test set\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa80074",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Len all : {X.shape} | X_train : {X_train.shape} | X_test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarize data\n",
    "from sklearn.preprocessing import MinMaxScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d31f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([5,10,15,20])/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81289642",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1748ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler_object.transform(X_train)\n",
    "scaled_X_test =  scaler_object.transform(X_test)\n",
    "scaled_X_train[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b0e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_test[-3:] #all values are between 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "        #      8 neurons 4 features\n",
    "model.add(Dense(8,input_dim=4,activation='relu'))#add layers\n",
    "model.add(Dense(8,input_dim=4,activation='relu'))#add layers\n",
    "model.add(Dense(3,activation='softmax'))#add layers[0.2,0.3,0.5]\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf285031",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(scaled_X_train,y_train,epochs=150,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_x=model.predict(scaled_X_test)\n",
    "classes_x=np.argmax(predict_x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0cbace",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df34197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68255f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test.argmax(axis=1),classes_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test.argmax(axis=1),classes_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89daa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test.argmax(axis=1),classes_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('myfirstmodel.h5')#this save all the weigths of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fccd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load the model\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956db59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model =load_model('myfirstmodel.h5')#be careful with name to no overwrtie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e092823",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=new_model.predict(scaled_X_test)\n",
    "classes = np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4a8d7",
   "metadata": {},
   "source": [
    "## Text Generation with LSTM (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ea1818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text=f.read()\n",
    "    \n",
    "    return str_text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78bb4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca727eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and clean\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg',disable=['parser','tagger','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "410ac24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1ed9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addbcea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71698279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\VirtualEnvs\\NLP_course\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb95c1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11338"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6030c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sequence of tokens\n",
    "#25 words --> network predict #26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a87e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 25 + 1\n",
    "text_sequences = []\n",
    "for i in range(train_len,len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27db3926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09bb9733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f84ce92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e54eac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48dfa3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c28baec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1cd41e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[956,\n",
       " 14,\n",
       " 263,\n",
       " 51,\n",
       " 261,\n",
       " 408,\n",
       " 87,\n",
       " 219,\n",
       " 129,\n",
       " 111,\n",
       " 954,\n",
       " 260,\n",
       " 50,\n",
       " 43,\n",
       " 38,\n",
       " 314,\n",
       " 7,\n",
       " 23,\n",
       " 546,\n",
       " 3,\n",
       " 150,\n",
       " 259,\n",
       " 6,\n",
       " 2713,\n",
       " 14,\n",
       " 24]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "845456d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "956 : call\n",
      "14 : me\n",
      "263 : ishmael\n",
      "51 : some\n",
      "261 : years\n",
      "408 : ago\n",
      "87 : never\n",
      "219 : mind\n",
      "129 : how\n",
      "111 : long\n",
      "954 : precisely\n",
      "260 : having\n",
      "50 : little\n",
      "43 : or\n",
      "38 : no\n",
      "314 : money\n",
      "7 : in\n",
      "23 : my\n",
      "546 : purse\n",
      "3 : and\n",
      "150 : nothing\n",
      "259 : particular\n",
      "6 : to\n",
      "2713 : interest\n",
      "14 : me\n",
      "24 : on\n"
     ]
    }
   ],
   "source": [
    "for i in sequences[0]:\n",
    "    print(f\"{i} : {tokenizer.index_word[i]}\")\n",
    "# tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06fdaf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('call', 27),\n",
       "             ('me', 2471),\n",
       "             ('ishmael', 133),\n",
       "             ('some', 758),\n",
       "             ('years', 135),\n",
       "             ('ago', 84),\n",
       "             ('never', 449),\n",
       "             ('mind', 164),\n",
       "             ('how', 321),\n",
       "             ('long', 374),\n",
       "             ('precisely', 37),\n",
       "             ('having', 142),\n",
       "             ('little', 767),\n",
       "             ('or', 950),\n",
       "             ('no', 1003),\n",
       "             ('money', 120),\n",
       "             ('in', 5647),\n",
       "             ('my', 1786),\n",
       "             ('purse', 71),\n",
       "             ('and', 9646),\n",
       "             ('nothing', 281),\n",
       "             ('particular', 152),\n",
       "             ('to', 6497),\n",
       "             ('interest', 24),\n",
       "             ('on', 1716),\n",
       "             ('shore', 26),\n",
       "             ('i', 7150),\n",
       "             ('thought', 676),\n",
       "             ('would', 702),\n",
       "             ('sail', 104),\n",
       "             ('about', 1014),\n",
       "             ('a', 10377),\n",
       "             ('see', 416),\n",
       "             ('the', 15540),\n",
       "             ('watery', 26),\n",
       "             ('part', 234),\n",
       "             ('of', 8287),\n",
       "             ('world', 234),\n",
       "             ('it', 4238),\n",
       "             ('is', 1950),\n",
       "             ('way', 390),\n",
       "             ('have', 806),\n",
       "             ('driving', 26),\n",
       "             ('off', 416),\n",
       "             ('spleen', 26),\n",
       "             ('regulating', 26),\n",
       "             ('circulation', 26),\n",
       "             ('whenever', 130),\n",
       "             ('find', 78),\n",
       "             ('myself', 416),\n",
       "             ('growing', 26),\n",
       "             ('grim', 26),\n",
       "             ('mouth', 130),\n",
       "             ('damp', 78),\n",
       "             ('drizzly', 26),\n",
       "             ('november', 26),\n",
       "             ('soul', 78),\n",
       "             ('involuntarily', 52),\n",
       "             ('pausing', 52),\n",
       "             ('before', 364),\n",
       "             ('coffin', 104),\n",
       "             ('warehouses', 52),\n",
       "             ('bringing', 26),\n",
       "             ('up', 1237),\n",
       "             ('rear', 26),\n",
       "             ('every', 182),\n",
       "             ('funeral', 26),\n",
       "             ('meet', 26),\n",
       "             ('especially', 104),\n",
       "             ('hypos', 26),\n",
       "             ('get', 364),\n",
       "             ('such', 572),\n",
       "             ('an', 806),\n",
       "             ('upper', 26),\n",
       "             ('hand', 312),\n",
       "             ('that', 3770),\n",
       "             ('requires', 52),\n",
       "             ('strong', 78),\n",
       "             ('moral', 26),\n",
       "             ('principle', 26),\n",
       "             ('prevent', 26),\n",
       "             ('from', 1508),\n",
       "             ('deliberately', 26),\n",
       "             ('stepping', 26),\n",
       "             ('into', 988),\n",
       "             ('street', 104),\n",
       "             ('methodically', 26),\n",
       "             ('knocking', 26),\n",
       "             ('people', 52),\n",
       "             (\"'s\", 1691),\n",
       "             ('hats', 26),\n",
       "             ('then', 832),\n",
       "             ('account', 78),\n",
       "             ('high', 130),\n",
       "             ('time', 520),\n",
       "             ('sea', 546),\n",
       "             ('as', 2366),\n",
       "             ('soon', 234),\n",
       "             ('can', 338),\n",
       "             ('this', 2158),\n",
       "             ('substitute', 26),\n",
       "             ('for', 1820),\n",
       "             ('pistol', 26),\n",
       "             ('ball', 26),\n",
       "             ('with', 2392),\n",
       "             ('philosophical', 26),\n",
       "             ('flourish', 26),\n",
       "             ('cato', 26),\n",
       "             ('throws', 26),\n",
       "             ('himself', 338),\n",
       "             ('upon', 780),\n",
       "             ('his', 3139),\n",
       "             ('sword', 78),\n",
       "             ('quietly', 78),\n",
       "             ('take', 260),\n",
       "             ('ship', 182),\n",
       "             ('there', 1456),\n",
       "             ('surprising', 26),\n",
       "             ('if', 728),\n",
       "             ('they', 728),\n",
       "             ('but', 2652),\n",
       "             ('knew', 130),\n",
       "             ('almost', 286),\n",
       "             ('all', 1872),\n",
       "             ('men', 130),\n",
       "             ('their', 390),\n",
       "             ('degree', 78),\n",
       "             ('other', 494),\n",
       "             ('cherish', 26),\n",
       "             ('very', 494),\n",
       "             ('nearly', 52),\n",
       "             ('same', 312),\n",
       "             ('feelings', 26),\n",
       "             ('towards', 260),\n",
       "             ('ocean', 52),\n",
       "             ('now', 1040),\n",
       "             ('your', 442),\n",
       "             ('insular', 26),\n",
       "             ('city', 104),\n",
       "             ('manhattoes', 26),\n",
       "             ('belted', 26),\n",
       "             ('round', 364),\n",
       "             ('by', 962),\n",
       "             ('wharves', 26),\n",
       "             ('indian', 52),\n",
       "             ('isles', 26),\n",
       "             ('coral', 26),\n",
       "             ('reefs', 26),\n",
       "             ('commerce', 26),\n",
       "             ('surrounds', 26),\n",
       "             ('her', 156),\n",
       "             ('surf', 26),\n",
       "             ('right', 156),\n",
       "             ('left', 78),\n",
       "             ('streets', 208),\n",
       "             ('you', 2158),\n",
       "             ('waterward', 52),\n",
       "             ('its', 156),\n",
       "             ('extreme', 26),\n",
       "             ('downtown', 26),\n",
       "             ('battery', 52),\n",
       "             ('where', 364),\n",
       "             ('noble', 52),\n",
       "             ('mole', 26),\n",
       "             ('washed', 52),\n",
       "             ('waves', 26),\n",
       "             ('cooled', 26),\n",
       "             ('breezes', 26),\n",
       "             ('which', 572),\n",
       "             ('few', 104),\n",
       "             ('hours', 130),\n",
       "             ('previous', 104),\n",
       "             ('were', 962),\n",
       "             ('out', 956),\n",
       "             ('sight', 104),\n",
       "             ('land', 208),\n",
       "             ('look', 156),\n",
       "             ('at', 2184),\n",
       "             ('crowds', 52),\n",
       "             ('water', 234),\n",
       "             ('gazers', 26),\n",
       "             ('circumambulate', 26),\n",
       "             ('dreamy', 26),\n",
       "             ('sabbath', 52),\n",
       "             ('afternoon', 52),\n",
       "             ('go', 494),\n",
       "             ('corlears', 26),\n",
       "             ('hook', 26),\n",
       "             ('coenties', 26),\n",
       "             ('slip', 26),\n",
       "             ('thence', 52),\n",
       "             ('whitehall', 26),\n",
       "             ('northward', 26),\n",
       "             ('what', 1118),\n",
       "             ('do', 702),\n",
       "             ('see?--posted', 26),\n",
       "             ('like', 732),\n",
       "             ('silent', 52),\n",
       "             ('sentinels', 26),\n",
       "             ('around', 78),\n",
       "             ('town', 156),\n",
       "             ('stand', 182),\n",
       "             ('thousands', 52),\n",
       "             ('mortal', 26),\n",
       "             ('fixed', 78),\n",
       "             ('reveries', 52),\n",
       "             ('leaning', 52),\n",
       "             ('against', 234),\n",
       "             ('spiles', 26),\n",
       "             ('seated', 52),\n",
       "             ('pier', 26),\n",
       "             ('heads', 338),\n",
       "             ('looking', 312),\n",
       "             ('over', 702),\n",
       "             ('bulwarks', 52),\n",
       "             ('ships', 78),\n",
       "             ('china', 26),\n",
       "             ('aloft', 52),\n",
       "             ('rigging', 26),\n",
       "             ('striving', 26),\n",
       "             ('still', 364),\n",
       "             ('better', 208),\n",
       "             ('seaward', 26),\n",
       "             ('peep', 26),\n",
       "             ('these', 494),\n",
       "             ('are', 416),\n",
       "             ('landsmen', 26),\n",
       "             ('week', 52),\n",
       "             ('days', 104),\n",
       "             ('pent', 26),\n",
       "             ('lath', 26),\n",
       "             ('plaster', 52),\n",
       "             ('tied', 26),\n",
       "             ('counters', 26),\n",
       "             ('nailed', 26),\n",
       "             ('benches', 26),\n",
       "             ('clinched', 26),\n",
       "             ('desks', 26),\n",
       "             ('green', 130),\n",
       "             ('fields', 26),\n",
       "             ('gone', 52),\n",
       "             ('here', 598),\n",
       "             ('come', 338),\n",
       "             ('more', 494),\n",
       "             ('pacing', 26),\n",
       "             ('straight', 104),\n",
       "             ('seemingly', 26),\n",
       "             ('bound', 52),\n",
       "             ('dive', 26),\n",
       "             ('strange', 182),\n",
       "             ('will', 260),\n",
       "             ('content', 52),\n",
       "             ('them', 442),\n",
       "             ('extremest', 26),\n",
       "             ('limit', 26),\n",
       "             ('loitering', 26),\n",
       "             ('under', 260),\n",
       "             ('shady', 26),\n",
       "             ('lee', 26),\n",
       "             ('yonder', 52),\n",
       "             ('not', 1534),\n",
       "             ('suffice', 26),\n",
       "             ('must', 442),\n",
       "             ('just', 390),\n",
       "             ('nigh', 104),\n",
       "             ('possibly', 26),\n",
       "             ('without', 182),\n",
       "             ('falling', 52),\n",
       "             ('miles', 78),\n",
       "             ('leagues', 26),\n",
       "             ('inlanders', 26),\n",
       "             ('lanes', 26),\n",
       "             ('alleys', 26),\n",
       "             ('avenues', 26),\n",
       "             ('north', 52),\n",
       "             ('east', 26),\n",
       "             ('south', 156),\n",
       "             ('west', 26),\n",
       "             ('yet', 416),\n",
       "             ('unite', 26),\n",
       "             ('tell', 442),\n",
       "             ('does', 156),\n",
       "             ('magnetic', 26),\n",
       "             ('virtue', 26),\n",
       "             ('needles', 26),\n",
       "             ('compasses', 26),\n",
       "             ('those', 234),\n",
       "             ('attract', 26),\n",
       "             ('thither', 26),\n",
       "             ('once', 208),\n",
       "             ('say', 286),\n",
       "             ('country', 78),\n",
       "             ('lakes', 26),\n",
       "             ('any', 364),\n",
       "             ('path', 26),\n",
       "             ('please', 52),\n",
       "             ('ten', 52),\n",
       "             ('one', 1300),\n",
       "             ('carries', 26),\n",
       "             ('down', 468),\n",
       "             ('dale', 26),\n",
       "             ('leaves', 52),\n",
       "             ('pool', 26),\n",
       "             ('stream', 78),\n",
       "             ('magic', 52),\n",
       "             ('let', 156),\n",
       "             ('most', 468),\n",
       "             ('absent', 26),\n",
       "             ('minded', 26),\n",
       "             ('be', 1716),\n",
       "             ('plunged', 52),\n",
       "             ('deepest', 26),\n",
       "             ('man', 572),\n",
       "             ('legs', 104),\n",
       "             ('set', 156),\n",
       "             ('feet', 182),\n",
       "             ('going', 260),\n",
       "             ('he', 3247),\n",
       "             ('infallibly', 26),\n",
       "             ('lead', 78),\n",
       "             ('region', 26),\n",
       "             ('should', 286),\n",
       "             ('ever', 338),\n",
       "             ('athirst', 26),\n",
       "             ('great', 376),\n",
       "             ('american', 78),\n",
       "             ('desert', 26),\n",
       "             ('try', 104),\n",
       "             ('experiment', 26),\n",
       "             ('caravan', 26),\n",
       "             ('happen', 26),\n",
       "             ('supplied', 26),\n",
       "             ('metaphysical', 52),\n",
       "             ('professor', 26),\n",
       "             ('yes', 104),\n",
       "             ('knows', 26),\n",
       "             ('meditation', 26),\n",
       "             ('wedded', 26),\n",
       "             ('artist', 78),\n",
       "             ('desires', 26),\n",
       "             ('paint', 26),\n",
       "             ('dreamiest', 26),\n",
       "             ('shadiest', 26),\n",
       "             ('quietest', 26),\n",
       "             ('enchanting', 26),\n",
       "             ('bit', 130),\n",
       "             ('romantic', 26),\n",
       "             ('landscape', 26),\n",
       "             ('valley', 26),\n",
       "             ('saco', 26),\n",
       "             ('chief', 52),\n",
       "             ('element', 26),\n",
       "             ('employs', 26),\n",
       "             ('trees', 26),\n",
       "             ('each', 78),\n",
       "             ('hollow', 26),\n",
       "             ('trunk', 52),\n",
       "             ('hermit', 26),\n",
       "             ('crucifix', 26),\n",
       "             ('within', 130),\n",
       "             ('sleeps', 26),\n",
       "             ('meadow', 52),\n",
       "             ('sleep', 416),\n",
       "             ('cattle', 26),\n",
       "             ('cottage', 26),\n",
       "             ('goes', 78),\n",
       "             ('sleepy', 26),\n",
       "             ('smoke', 52),\n",
       "             ('deep', 78),\n",
       "             ('distant', 78),\n",
       "             ('woodlands', 26),\n",
       "             ('winds', 78),\n",
       "             ('mazy', 26),\n",
       "             ('reaching', 52),\n",
       "             ('overlapping', 26),\n",
       "             ('spurs', 26),\n",
       "             ('mountains', 26),\n",
       "             ('bathed', 26),\n",
       "             ('hill', 52),\n",
       "             ('side', 286),\n",
       "             ('blue', 78),\n",
       "             ('though', 494),\n",
       "             ('picture', 130),\n",
       "             ('lies', 26),\n",
       "             ('thus', 52),\n",
       "             ('tranced', 26),\n",
       "             ('pine', 52),\n",
       "             ('tree', 26),\n",
       "             ('shakes', 26),\n",
       "             ('sighs', 52),\n",
       "             ('shepherd', 52),\n",
       "             ('head', 598),\n",
       "             ('vain', 26),\n",
       "             ('unless', 104),\n",
       "             ('eye', 26),\n",
       "             ('him', 1092),\n",
       "             ('visit', 26),\n",
       "             ('prairies', 26),\n",
       "             ('june', 52),\n",
       "             ('when', 650),\n",
       "             ('scores', 52),\n",
       "             ('wade', 26),\n",
       "             ('knee', 26),\n",
       "             ('among', 78),\n",
       "             ('tiger', 26),\n",
       "             ('lilies', 26),\n",
       "             ('charm', 26),\n",
       "             ('wanting?--water', 26),\n",
       "             ('drop', 26),\n",
       "             ('niagara', 26),\n",
       "             ('cataract', 26),\n",
       "             ('sand', 26),\n",
       "             ('travel', 26),\n",
       "             ('thousand', 52),\n",
       "             ('why', 286),\n",
       "             ('did', 572),\n",
       "             ('poor', 104),\n",
       "             ('poet', 26),\n",
       "             ('tennessee', 26),\n",
       "             ('suddenly', 78),\n",
       "             ('receiving', 26),\n",
       "             ('two', 338),\n",
       "             ('handfuls', 26),\n",
       "             ('silver', 26),\n",
       "             ('deliberate', 26),\n",
       "             ('whether', 182),\n",
       "             ('buy', 26),\n",
       "             ('coat', 104),\n",
       "             ('sadly', 52),\n",
       "             ('needed', 26),\n",
       "             ('invest', 26),\n",
       "             ('pedestrian', 26),\n",
       "             ('trip', 26),\n",
       "             ('rockaway', 26),\n",
       "             ('beach', 26),\n",
       "             ('robust', 52),\n",
       "             ('healthy', 52),\n",
       "             ('boy', 52),\n",
       "             ('crazy', 52),\n",
       "             ('first', 494),\n",
       "             ('voyage', 208),\n",
       "             ('passenger', 104),\n",
       "             ('yourself', 156),\n",
       "             ('feel', 78),\n",
       "             ('mystical', 26),\n",
       "             ('vibration', 26),\n",
       "             ('told', 130),\n",
       "             ('old', 754),\n",
       "             ('persians', 26),\n",
       "             ('hold', 52),\n",
       "             ('holy', 52),\n",
       "             ('greeks', 26),\n",
       "             ('give', 208),\n",
       "             ('separate', 26),\n",
       "             ('deity', 26),\n",
       "             ('own', 286),\n",
       "             ('brother', 52),\n",
       "             ('jove', 26),\n",
       "             ('surely', 26),\n",
       "             ('meaning', 78),\n",
       "             ('deeper', 26),\n",
       "             ('story', 130),\n",
       "             ('narcissus', 26),\n",
       "             ('who', 416),\n",
       "             ('because', 182),\n",
       "             ('could', 650),\n",
       "             ('grasp', 26),\n",
       "             ('tormenting', 26),\n",
       "             ('mild', 26),\n",
       "             ('image', 156),\n",
       "             ('saw', 156),\n",
       "             ('fountain', 26),\n",
       "             ('was', 2886),\n",
       "             ('drowned', 26),\n",
       "             ('we', 286),\n",
       "             ('ourselves', 52),\n",
       "             ('rivers', 26),\n",
       "             ('oceans', 26),\n",
       "             ('ungraspable', 26),\n",
       "             ('phantom', 78),\n",
       "             ('life', 78),\n",
       "             ('key', 26),\n",
       "             ('am', 156),\n",
       "             ('habit', 26),\n",
       "             ('begin', 52),\n",
       "             ('grow', 52),\n",
       "             ('hazy', 26),\n",
       "             ('eyes', 182),\n",
       "             ('conscious', 26),\n",
       "             ('lungs', 26),\n",
       "             ('mean', 130),\n",
       "             ('inferred', 52),\n",
       "             ('needs', 52),\n",
       "             ('rag', 26),\n",
       "             ('something', 312),\n",
       "             ('besides', 156),\n",
       "             ('passengers', 78),\n",
       "             ('sick', 26),\n",
       "             ('quarrelsome', 26),\n",
       "             (\"don't\", 52),\n",
       "             ('nights', 26),\n",
       "             ('enjoy', 26),\n",
       "             ('themselves', 52),\n",
       "             ('much', 442),\n",
       "             ('general', 26),\n",
       "             ('thing;--no', 26),\n",
       "             ('nor', 78),\n",
       "             ('salt', 26),\n",
       "             ('commodore', 52),\n",
       "             ('captain', 52),\n",
       "             ('cook', 52),\n",
       "             ('abandon', 26),\n",
       "             ('glory', 52),\n",
       "             ('distinction', 26),\n",
       "             ('offices', 26),\n",
       "             ('abominate', 26),\n",
       "             ('honourable', 26),\n",
       "             ('respectable', 26),\n",
       "             ('toils', 26),\n",
       "             ('trials', 26),\n",
       "             ('tribulations', 26),\n",
       "             ('kind', 78),\n",
       "             ('whatsoever', 52),\n",
       "             ('quite', 78),\n",
       "             ('care', 78),\n",
       "             ('taking', 104),\n",
       "             ('barques', 26),\n",
       "             ('brigs', 26),\n",
       "             ('schooners', 26),\n",
       "             ('cook,--though', 26),\n",
       "             ('confess', 52),\n",
       "             ('considerable', 26),\n",
       "             ('being', 390),\n",
       "             ('sort', 494),\n",
       "             ('officer', 52),\n",
       "             ('board', 78),\n",
       "             ('somehow', 78),\n",
       "             ('fancied', 26),\n",
       "             ('broiling', 26),\n",
       "             ('fowls;--though', 26),\n",
       "             ('broiled', 78),\n",
       "             ('judiciously', 26),\n",
       "             ('buttered', 26),\n",
       "             ('judgmatically', 26),\n",
       "             ('salted', 26),\n",
       "             ('peppered', 26),\n",
       "             ('speak', 130),\n",
       "             ('respectfully', 52),\n",
       "             ('reverentially', 26),\n",
       "             ('fowl', 26),\n",
       "             ('than', 390),\n",
       "             ('idolatrous', 26),\n",
       "             ('dotings', 26),\n",
       "             ('egyptians', 26),\n",
       "             ('ibis', 26),\n",
       "             ('roasted', 26),\n",
       "             ('river', 26),\n",
       "             ('horse', 52),\n",
       "             ('mummies', 26),\n",
       "             ('creatures', 26),\n",
       "             ('huge', 52),\n",
       "             ('bake', 26),\n",
       "             ('houses', 52),\n",
       "             ('pyramids', 26),\n",
       "             ('simple', 26),\n",
       "             ('sailor', 156),\n",
       "             ('mast', 78),\n",
       "             ('plumb', 26),\n",
       "             ('forecastle', 52),\n",
       "             ('royal', 26),\n",
       "             ('true', 104),\n",
       "             ('rather', 208),\n",
       "             ('order', 130),\n",
       "             ('make', 260),\n",
       "             ('jump', 52),\n",
       "             ('spar', 52),\n",
       "             ('grasshopper', 26),\n",
       "             ('may', 312),\n",
       "             ('thing', 104),\n",
       "             ('unpleasant', 26),\n",
       "             ('enough', 338),\n",
       "             ('touches', 26),\n",
       "             ('sense', 78),\n",
       "             ('honour', 26),\n",
       "             ('particularly', 52),\n",
       "             ('established', 26),\n",
       "             ('family', 26),\n",
       "             ('van', 26),\n",
       "             ('rensselaers', 26),\n",
       "             ('randolphs', 26),\n",
       "             ('hardicanutes', 26),\n",
       "             ('putting', 52),\n",
       "             ('tar', 52),\n",
       "             ('pot', 26),\n",
       "             ('been', 468),\n",
       "             ('lording', 26),\n",
       "             ('schoolmaster', 52),\n",
       "             ('making', 130),\n",
       "             ('tallest', 26),\n",
       "             ('boys', 52),\n",
       "             ('awe', 26),\n",
       "             ('transition', 52),\n",
       "             ('keen', 26),\n",
       "             ('assure', 26),\n",
       "             ('decoction', 26),\n",
       "             ('seneca', 26),\n",
       "             ('stoics', 26),\n",
       "             ('enable', 26),\n",
       "             ('grin', 52),\n",
       "             ('bear', 52),\n",
       "             ('even', 130),\n",
       "             ('wears', 26),\n",
       "             ('hunks', 52),\n",
       "             ('orders', 26),\n",
       "             ('broom', 26),\n",
       "             ('sweep', 52),\n",
       "             ('decks', 26),\n",
       "             ('indignity', 26),\n",
       "             ('amount', 26),\n",
       "             ('weighed', 52),\n",
       "             ('scales', 26),\n",
       "             ('new', 286),\n",
       "             ('testament', 26),\n",
       "             ('think', 182),\n",
       "             ('archangel', 26),\n",
       "             ('gabriel', 26),\n",
       "             ('thinks', 182),\n",
       "             ('anything', 52),\n",
       "             ('less', 52),\n",
       "             ('promptly', 26),\n",
       "             ('obey', 26),\n",
       "             ('instance', 26),\n",
       "             ('ai', 104),\n",
       "             (\"n't\", 624),\n",
       "             ('slave', 26),\n",
       "             ('well', 208),\n",
       "             ('however', 208),\n",
       "             ('captains', 26),\n",
       "             ('thump', 52),\n",
       "             ('punch', 26),\n",
       "             ('satisfaction', 26),\n",
       "             ('knowing', 78),\n",
       "             ('everybody', 26),\n",
       "             ('else', 208),\n",
       "             ('served', 26),\n",
       "             ('either', 78),\n",
       "             ('physical', 26),\n",
       "             ('point', 52),\n",
       "             ('view', 52),\n",
       "             ('so', 1066),\n",
       "             ('universal', 26),\n",
       "             ('passed', 78),\n",
       "             ('hands', 78),\n",
       "             ('rub', 26),\n",
       "             ('shoulder', 26),\n",
       "             ('blades', 26),\n",
       "             ('again', 286),\n",
       "             ('always', 104),\n",
       "             ('paying', 78),\n",
       "             ('trouble', 26),\n",
       "             ('whereas', 26),\n",
       "             ('pay', 78),\n",
       "             ('single', 52),\n",
       "             ('penny', 78),\n",
       "             ('heard', 208),\n",
       "             ('contrary', 26),\n",
       "             ('difference', 52),\n",
       "             ('between', 234),\n",
       "             ('paid', 26),\n",
       "             ('act', 52),\n",
       "             ('perhaps', 130),\n",
       "             ('uncomfortable', 52),\n",
       "             ('infliction', 26),\n",
       "             ('orchard', 26),\n",
       "             ('thieves', 26),\n",
       "             ('entailed', 26),\n",
       "             ('us', 104),\n",
       "             ('paid,--what', 26),\n",
       "             ('compare', 52),\n",
       "             ('urbane', 26),\n",
       "             ('activity', 26),\n",
       "             ('receives', 26),\n",
       "             ('really', 104),\n",
       "             ('marvellous', 104),\n",
       "             ('considering', 26),\n",
       "             ('earnestly', 26),\n",
       "             ('believe', 26),\n",
       "             ('root', 26),\n",
       "             ('earthly', 52),\n",
       "             ('ills', 26),\n",
       "             ('monied', 26),\n",
       "             ('enter', 52),\n",
       "             ('heaven', 104),\n",
       "             ('ah', 26),\n",
       "             ('cheerfully', 26),\n",
       "             ('consign', 26),\n",
       "             ('perdition', 26),\n",
       "             ('finally', 26),\n",
       "             ('wholesome', 26),\n",
       "             ('exercise', 26),\n",
       "             ('pure', 26),\n",
       "             ('air', 104),\n",
       "             ('fore', 26),\n",
       "             ('castle', 26),\n",
       "             ('deck', 52),\n",
       "             ('far', 104),\n",
       "             ('prevalent', 26),\n",
       "             ('astern', 26),\n",
       "             ('violate', 26),\n",
       "             ('pythagorean', 26),\n",
       "             ('maxim', 26),\n",
       "             ('quarter', 52),\n",
       "             ('gets', 26),\n",
       "             ('atmosphere', 26),\n",
       "             ('second', 104),\n",
       "             ('sailors', 78),\n",
       "             ('breathes', 26),\n",
       "             ('commonalty', 26),\n",
       "             ('leaders', 52),\n",
       "             ('many', 104),\n",
       "             ('things', 130),\n",
       "             ('suspect', 26),\n",
       "             ('wherefore', 26),\n",
       "             ('after', 234),\n",
       "             ('repeatedly', 26),\n",
       "             ('smelt', 52),\n",
       "             ('merchant', 26),\n",
       "             ('whaling', 234),\n",
       "             ('invisible', 26),\n",
       "             ('police', 26),\n",
       "             ('fates', 52),\n",
       "             ('has', 104),\n",
       "             ('constant', 26),\n",
       "             ('surveillance', 26),\n",
       "             ('secretly', 26),\n",
       "             ('dogs', 26),\n",
       "             ('influences', 26),\n",
       "             ('unaccountable', 104),\n",
       "             ('answer', 130),\n",
       "             ('doubtless', 52),\n",
       "             ('formed', 52),\n",
       "             ('grand', 104),\n",
       "             ('programme', 26),\n",
       "             ('providence', 26),\n",
       "             ('drawn', 26),\n",
       "             ('came', 286),\n",
       "             ('brief', 26),\n",
       "             ('interlude', 26),\n",
       "             ('solo', 26),\n",
       "             ('extensive', 26),\n",
       "             ('performances', 26),\n",
       "             ('bill', 26),\n",
       "             ('run', 52),\n",
       "             ('contested', 26),\n",
       "             ('election', 26),\n",
       "             ('presidency', 26),\n",
       "             ('united', 26),\n",
       "             ('states', 26),\n",
       "             ('bloody', 26),\n",
       "             ('battle', 26),\n",
       "             ('affghanistan', 26),\n",
       "             ('exactly', 78),\n",
       "             ('stage', 52),\n",
       "             ('managers', 26),\n",
       "             ('put', 208),\n",
       "             ('shabby', 52),\n",
       "             ('others', 52),\n",
       "             ('magnificent', 26),\n",
       "             ('parts', 130),\n",
       "             ('tragedies', 26),\n",
       "             ('short', 78),\n",
       "             ('easy', 78),\n",
       "             ('genteel', 26),\n",
       "             ('comedies', 26),\n",
       "             ('jolly', 104),\n",
       "             ('farces', 26),\n",
       "             ('recall', 26),\n",
       "             ('circumstances', 52),\n",
       "             ('springs', 26),\n",
       "             ('motives', 52),\n",
       "             ('cunningly', 26),\n",
       "             ('presented', 26),\n",
       "             ('various', 52),\n",
       "             ('disguises', 26),\n",
       "             ('induced', 26),\n",
       "             ('performing', 26),\n",
       "             ('cajoling', 26),\n",
       "             ('delusion', 26),\n",
       "             ('choice', 26),\n",
       "             ('resulting', 26),\n",
       "             ('unbiased', 26),\n",
       "             ('freewill', 26),\n",
       "             ('discriminating', 26),\n",
       "             ('judgment', 26),\n",
       "             ('overwhelming', 26),\n",
       "             ('idea', 182),\n",
       "             ('whale', 260),\n",
       "             ('portentous', 78),\n",
       "             ('mysterious', 52),\n",
       "             ('monster', 26),\n",
       "             ('roused', 26),\n",
       "             ('curiosity', 52),\n",
       "             ('wild', 130),\n",
       "             ('seas', 156),\n",
       "             ('rolled', 156),\n",
       "             ('island', 78),\n",
       "             ('bulk', 26),\n",
       "             ('undeliverable', 26),\n",
       "             ('nameless', 78),\n",
       "             ('perils', 26),\n",
       "             ('attending', 26),\n",
       "             ('marvels', 26),\n",
       "             ('patagonian', 26),\n",
       "             ('sights', 26),\n",
       "             ('sounds', 78),\n",
       "             ('helped', 26),\n",
       "             ('sway', 26),\n",
       "             ('wish', 26),\n",
       "             ('inducements', 26),\n",
       "             ('tormented', 52),\n",
       "             ('everlasting', 52),\n",
       "             ('itch', 26),\n",
       "             ('remote', 26),\n",
       "             ('love', 26),\n",
       "             ('forbidden', 26),\n",
       "             ('barbarous', 26),\n",
       "             ('coasts', 26),\n",
       "             ('ignoring', 26),\n",
       "             ('good', 442),\n",
       "             ('quick', 26),\n",
       "             ('perceive', 26),\n",
       "             ('horror', 26),\n",
       "             ('social', 26),\n",
       "             ('since', 78),\n",
       "             ('friendly', 26),\n",
       "             ('terms', 26),\n",
       "             ('inmates', 26),\n",
       "             ('place', 390),\n",
       "             ('lodges', 26),\n",
       "             ('reason', 130),\n",
       "             ('welcome', 26),\n",
       "             ('flood', 26),\n",
       "             ('gates', 26),\n",
       "             ('wonder', 52),\n",
       "             ('swung', 26),\n",
       "             ('open', 104),\n",
       "             ('conceits', 26),\n",
       "             ('swayed', 26),\n",
       "             ('purpose', 52),\n",
       "             ('floated', 52),\n",
       "             ('inmost', 26),\n",
       "             ('endless', 26),\n",
       "             ('processions', 26),\n",
       "             ('mid', 26),\n",
       "             ('hooded', 26),\n",
       "             ('snow', 78),\n",
       "             ('stuffed', 52),\n",
       "             ('shirt', 104),\n",
       "             ('carpet', 26),\n",
       "             ('bag', 182),\n",
       "             ('tucked', 26),\n",
       "             ('arm', 286),\n",
       "             ('started', 26),\n",
       "             ('cape', 104),\n",
       "             ('horn', 52),\n",
       "             ('pacific', 26),\n",
       "             ('quitting', 26),\n",
       "             ('manhatto', 26),\n",
       "             ('duly', 26),\n",
       "             ('arrived', 52),\n",
       "             ('bedford', 104),\n",
       "             ('saturday', 78),\n",
       "             ('night', 624),\n",
       "             ('december', 26),\n",
       "             ('disappointed', 26),\n",
       "             ('learning', 26),\n",
       "             ('packet', 26),\n",
       "             ('nantucket', 182),\n",
       "             ('had', 858),\n",
       "             ('already', 26),\n",
       "             ('sailed', 26),\n",
       "             ('offer', 52),\n",
       "             ('till', 156),\n",
       "             ('following', 52),\n",
       "             ('monday', 26),\n",
       "             ('young', 130),\n",
       "             ('candidates', 26),\n",
       "             ('pains', 26),\n",
       "             ('penalties', 26),\n",
       "             ('stop', 208),\n",
       "             ('embark', 52),\n",
       "             ('related', 26),\n",
       "             ('doing', 26),\n",
       "             ('made', 338),\n",
       "             ('craft', 130),\n",
       "             ('fine', 104),\n",
       "             ('boisterous', 26),\n",
       "             ('everything', 26),\n",
       "             ('connected', 26),\n",
       "             ('famous', 26),\n",
       "             ('amazingly', 26),\n",
       "             ('pleased', 52),\n",
       "             ('late', 182),\n",
       "             ('gradually', 26),\n",
       "             ('monopolising', 26),\n",
       "             ('business', 130),\n",
       "             ('matter', 78),\n",
       "             ('behind', 26),\n",
       "             ('original', 52),\n",
       "             ('tyre', 26),\n",
       "             ('carthage;--the', 26),\n",
       "             ('dead', 130),\n",
       "             ('stranded', 52),\n",
       "             ('aboriginal', 26),\n",
       "             ('whalemen', 26),\n",
       "             ('red', 78),\n",
       "             ('sally', 26),\n",
       "             ('canoes', 26),\n",
       "             ('chase', 26),\n",
       "             ('leviathan', 52),\n",
       "             ('too', 364),\n",
       "             ('adventurous', 26),\n",
       "             ('sloop', 26),\n",
       "             ('forth', 26),\n",
       "             ('partly', 78),\n",
       "             ('laden', 26),\n",
       "             ('imported', 26),\n",
       "             ('cobblestones', 26),\n",
       "             ('throw', 26),\n",
       "             ('whales', 52),\n",
       "             ('discover', 26),\n",
       "             ('risk', 26),\n",
       "             ('harpoon', 135),\n",
       "             ('bowsprit', 26),\n",
       "             ('day', 156),\n",
       "             ('another', 130),\n",
       "             ('ere', 78),\n",
       "             ('destined', 26),\n",
       "             ('port', 26),\n",
       "             ('became', 78),\n",
       "             ('concernment', 26),\n",
       "             ('eat', 26),\n",
       "             ('meanwhile', 78),\n",
       "             ('dubious', 26),\n",
       "             ('nay', 52),\n",
       "             ('dark', 208),\n",
       "             ('dismal', 52),\n",
       "             ('bitingly', 26),\n",
       "             ('cold', 182),\n",
       "             ('cheerless', 26),\n",
       "             ('anxious', 26),\n",
       "             ('grapnels', 26),\n",
       "             ('sounded', 26),\n",
       "             ('pocket', 78),\n",
       "             ('only', 364),\n",
       "             ('brought', 26),\n",
       "             ('pieces', 26),\n",
       "             ('silver,--so', 26),\n",
       "             ('wherever', 52),\n",
       "             ('said', 494),\n",
       "             ('stood', 260),\n",
       "             ('middle', 130),\n",
       "             ('dreary', 52),\n",
       "             ('shouldering', 26),\n",
       "             ('comparing', 26),\n",
       "             ('gloom', 26),\n",
       "             ('darkness', 78),\n",
       "             ('wisdom', 26),\n",
       "             ('conclude', 26),\n",
       "             ('lodge', 26),\n",
       "             ('dear', 26),\n",
       "             ('sure', 130),\n",
       "             ('inquire', 26),\n",
       "             ('price', 26),\n",
       "             ('halting', 26),\n",
       "             ('steps', 26),\n",
       "             ('paced', 26),\n",
       "             ('sign', 156),\n",
       "             ('crossed', 52),\n",
       "             ('harpoons\"--but', 26),\n",
       "             ('looked', 156),\n",
       "             ('expensive', 52),\n",
       "             ('further', 104),\n",
       "             ('bright', 52),\n",
       "             ('windows', 52),\n",
       "             ('fish', 78),\n",
       "             ('inn', 78),\n",
       "             ('fervent', 26),\n",
       "             ('rays', 26),\n",
       "             ('seemed', 416),\n",
       "             ('melted', 26),\n",
       "             ('packed', 52),\n",
       "             ('ice', 104),\n",
       "             ('house', 286),\n",
       "             ('everywhere', 26),\n",
       "             ('congealed', 26),\n",
       "             ('frost', 104),\n",
       "             ('lay', 234),\n",
       "             ('inches', 52),\n",
       "             ('thick', 52),\n",
       "             ...])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts#gives a dict of the num of eache word appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39819955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2718"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "vocabulary_size#unique words across the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9f839cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3b7008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 956,   14,  263, ..., 2713,   14,   24],\n",
       "       [  14,  263,   51, ...,   14,   24,  957],\n",
       "       [ 263,   51,  261, ...,   24,  957,    5],\n",
       "       ...,\n",
       "       [ 952,   12,  166, ...,  262,   53,    2],\n",
       "       [  12,  166, 2712, ...,   53,    2, 2718],\n",
       "       [ 166, 2712,    3, ...,    2, 2718,   26]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sequences = np.array(sequences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "590e67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into test & train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09f125f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 956,   14,  263, ...,    6, 2713,   14],\n",
       "       [  14,  263,   51, ..., 2713,   14,   24],\n",
       "       [ 263,   51,  261, ...,   14,   24,  957],\n",
       "       ...,\n",
       "       [ 952,   12,  166, ...,   11,  262,   53],\n",
       "       [  12,  166, 2712, ...,  262,   53,    2],\n",
       "       [ 166, 2712,    3, ...,   53,    2, 2718]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "sequences[:,:-1]#grab all except for the last columns, this column will be our label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0e6fdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  24,  957,    5, ...,    2, 2718,   26])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:,-1]#here we have just the last value for each column, this will be our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e052d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sequences[:,:-1]\n",
    "y=sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fbe08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y,num_classes=vocabulary_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1149d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21369af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bc04ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             50 NUMBER OF NEURONS, it recommend to be seq_len factor like seq_le*2 but this requires more time to train\n",
    "\n",
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce243d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bfec228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 25)            67975     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 25, 150)           105600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 150)               180600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2719)              410569    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 787,394\n",
      "Trainable params: 787,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=create_model(vocabulary_size+1,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "272f11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d3b316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "89/89 [==============================] - 10s 82ms/step - loss: 6.8448 - accuracy: 0.0468\n",
      "Epoch 2/200\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 6.3861 - accuracy: 0.0529\n",
      "Epoch 3/200\n",
      "89/89 [==============================] - 8s 87ms/step - loss: 6.3403 - accuracy: 0.0508\n",
      "Epoch 4/200\n",
      "89/89 [==============================] - 7s 83ms/step - loss: 6.2042 - accuracy: 0.0529\n",
      "Epoch 5/200\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 6.1063 - accuracy: 0.0530\n",
      "Epoch 6/200\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 5.9713 - accuracy: 0.0639\n",
      "Epoch 7/200\n",
      "89/89 [==============================] - 7s 79ms/step - loss: 5.8511 - accuracy: 0.0672\n",
      "Epoch 8/200\n",
      "89/89 [==============================] - 7s 79ms/step - loss: 5.7383 - accuracy: 0.0706\n",
      "Epoch 9/200\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 5.6571 - accuracy: 0.0711\n",
      "Epoch 10/200\n",
      "89/89 [==============================] - 7s 78ms/step - loss: 5.5883 - accuracy: 0.0729\n",
      "Epoch 11/200\n",
      "89/89 [==============================] - 7s 78ms/step - loss: 5.5269 - accuracy: 0.0751\n",
      "Epoch 12/200\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 5.4626 - accuracy: 0.0782\n",
      "Epoch 13/200\n",
      "89/89 [==============================] - 7s 79ms/step - loss: 5.4034 - accuracy: 0.0803\n",
      "Epoch 14/200\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 5.3448 - accuracy: 0.0809\n",
      "Epoch 15/200\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 5.2842 - accuracy: 0.0863\n",
      "Epoch 16/200\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 5.2280 - accuracy: 0.0879\n",
      "Epoch 17/200\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 5.1712 - accuracy: 0.0904\n",
      "Epoch 18/200\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 5.1123 - accuracy: 0.0936\n",
      "Epoch 19/200\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 5.0542 - accuracy: 0.0957\n",
      "Epoch 20/200\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 4.9972 - accuracy: 0.0965\n",
      "Epoch 21/200\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 4.9411 - accuracy: 0.0982\n",
      "Epoch 22/200\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 4.8912 - accuracy: 0.1010\n",
      "Epoch 23/200\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 4.8524 - accuracy: 0.0986\n",
      "Epoch 24/200\n",
      "89/89 [==============================] - 7s 83ms/step - loss: 4.7945 - accuracy: 0.1029\n",
      "Epoch 25/200\n",
      "89/89 [==============================] - 7s 83ms/step - loss: 4.7434 - accuracy: 0.1056\n",
      "Epoch 26/200\n",
      "89/89 [==============================] - 7s 84ms/step - loss: 4.6990 - accuracy: 0.1064\n",
      "Epoch 27/200\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 4.6540 - accuracy: 0.1043\n",
      "Epoch 28/200\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 4.6049 - accuracy: 0.1116\n",
      "Epoch 29/200\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 4.5627 - accuracy: 0.1083\n",
      "Epoch 30/200\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 4.5142 - accuracy: 0.1126\n",
      "Epoch 31/200\n",
      "89/89 [==============================] - 7s 83ms/step - loss: 4.4754 - accuracy: 0.1141\n",
      "Epoch 32/200\n",
      "89/89 [==============================] - 8s 85ms/step - loss: 4.4352 - accuracy: 0.1174\n",
      "Epoch 33/200\n",
      "89/89 [==============================] - 8s 95ms/step - loss: 4.3908 - accuracy: 0.1182\n",
      "Epoch 34/200\n",
      "89/89 [==============================] - 9s 102ms/step - loss: 4.3544 - accuracy: 0.1216\n",
      "Epoch 35/200\n",
      "89/89 [==============================] - 9s 103ms/step - loss: 4.3226 - accuracy: 0.1208\n",
      "Epoch 36/200\n",
      "89/89 [==============================] - 9s 103ms/step - loss: 4.2856 - accuracy: 0.1240\n",
      "Epoch 37/200\n",
      "89/89 [==============================] - 9s 102ms/step - loss: 4.2500 - accuracy: 0.1262\n",
      "Epoch 38/200\n",
      "89/89 [==============================] - 9s 106ms/step - loss: 4.5002 - accuracy: 0.1178\n",
      "Epoch 39/200\n",
      "89/89 [==============================] - 10s 109ms/step - loss: 5.2055 - accuracy: 0.0911\n",
      "Epoch 40/200\n",
      "89/89 [==============================] - 10s 112ms/step - loss: 4.7604 - accuracy: 0.1074\n",
      "Epoch 41/200\n",
      "89/89 [==============================] - 9s 105ms/step - loss: 4.4816 - accuracy: 0.1174\n",
      "Epoch 42/200\n",
      "89/89 [==============================] - 9s 102ms/step - loss: 4.3295 - accuracy: 0.1257\n",
      "Epoch 43/200\n",
      "89/89 [==============================] - 10s 110ms/step - loss: 4.2323 - accuracy: 0.1300\n",
      "Epoch 44/200\n",
      "89/89 [==============================] - 10s 107ms/step - loss: 4.1628 - accuracy: 0.1323\n",
      "Epoch 45/200\n",
      "89/89 [==============================] - 10s 109ms/step - loss: 4.1034 - accuracy: 0.1341\n",
      "Epoch 46/200\n",
      "89/89 [==============================] - 10s 111ms/step - loss: 4.0667 - accuracy: 0.1351\n",
      "Epoch 47/200\n",
      "89/89 [==============================] - 10s 116ms/step - loss: 4.0549 - accuracy: 0.1392\n",
      "Epoch 48/200\n",
      "89/89 [==============================] - 9s 101ms/step - loss: 4.0248 - accuracy: 0.1383\n",
      "Epoch 49/200\n",
      "89/89 [==============================] - 10s 115ms/step - loss: 3.9596 - accuracy: 0.1414\n",
      "Epoch 50/200\n",
      "89/89 [==============================] - 9s 106ms/step - loss: 3.8913 - accuracy: 0.1513\n",
      "Epoch 51/200\n",
      "89/89 [==============================] - 10s 108ms/step - loss: 3.8395 - accuracy: 0.1556\n",
      "Epoch 52/200\n",
      "89/89 [==============================] - 10s 107ms/step - loss: 3.7799 - accuracy: 0.1609\n",
      "Epoch 53/200\n",
      "89/89 [==============================] - 12s 134ms/step - loss: 3.7327 - accuracy: 0.1652\n",
      "Epoch 54/200\n",
      "89/89 [==============================] - 12s 134ms/step - loss: 3.6727 - accuracy: 0.1674\n",
      "Epoch 55/200\n",
      "89/89 [==============================] - 12s 133ms/step - loss: 3.6228 - accuracy: 0.1765\n",
      "Epoch 56/200\n",
      "89/89 [==============================] - 10s 113ms/step - loss: 3.5781 - accuracy: 0.1828\n",
      "Epoch 57/200\n",
      "89/89 [==============================] - 10s 111ms/step - loss: 3.5784 - accuracy: 0.1832\n",
      "Epoch 58/200\n",
      "89/89 [==============================] - 10s 116ms/step - loss: 3.5098 - accuracy: 0.1925\n",
      "Epoch 59/200\n",
      "89/89 [==============================] - 14s 154ms/step - loss: 3.4484 - accuracy: 0.1974\n",
      "Epoch 60/200\n",
      "89/89 [==============================] - 13s 149ms/step - loss: 3.3931 - accuracy: 0.2094\n",
      "Epoch 61/200\n",
      "89/89 [==============================] - 10s 117ms/step - loss: 3.3354 - accuracy: 0.2152\n",
      "Epoch 62/200\n",
      "89/89 [==============================] - 10s 109ms/step - loss: 3.3015 - accuracy: 0.2180\n",
      "Epoch 63/200\n",
      "89/89 [==============================] - 10s 110ms/step - loss: 3.2466 - accuracy: 0.2309\n",
      "Epoch 64/200\n",
      "89/89 [==============================] - 7s 84ms/step - loss: 3.2952 - accuracy: 0.2336\n",
      "Epoch 65/200\n",
      "89/89 [==============================] - 8s 85ms/step - loss: 3.3799 - accuracy: 0.2243\n",
      "Epoch 66/200\n",
      "89/89 [==============================] - 8s 88ms/step - loss: 3.3291 - accuracy: 0.2318\n",
      "Epoch 67/200\n",
      "89/89 [==============================] - 8s 89ms/step - loss: 3.2549 - accuracy: 0.2403\n",
      "Epoch 68/200\n",
      "89/89 [==============================] - 8s 93ms/step - loss: 3.1915 - accuracy: 0.2543\n",
      "Epoch 69/200\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 3.1058 - accuracy: 0.2612\n",
      "Epoch 70/200\n",
      "89/89 [==============================] - 8s 85ms/step - loss: 3.0493 - accuracy: 0.2706\n",
      "Epoch 71/200\n",
      "89/89 [==============================] - 9s 97ms/step - loss: 3.0180 - accuracy: 0.2810\n",
      "Epoch 72/200\n",
      "89/89 [==============================] - 8s 89ms/step - loss: 3.0734 - accuracy: 0.2775\n",
      "Epoch 73/200\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 3.0725 - accuracy: 0.2757\n",
      "Epoch 74/200\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 2.9264 - accuracy: 0.2955\n",
      "Epoch 75/200\n",
      "89/89 [==============================] - 9s 102ms/step - loss: 2.8453 - accuracy: 0.3151\n",
      "Epoch 76/200\n",
      "89/89 [==============================] - 10s 109ms/step - loss: 2.8606 - accuracy: 0.3117\n",
      "Epoch 77/200\n",
      "89/89 [==============================] - 10s 112ms/step - loss: 3.0156 - accuracy: 0.2999\n",
      "Epoch 78/200\n",
      "89/89 [==============================] - 10s 117ms/step - loss: 2.9341 - accuracy: 0.3093\n",
      "Epoch 79/200\n",
      "89/89 [==============================] - 9s 96ms/step - loss: 2.9532 - accuracy: 0.3047\n",
      "Epoch 80/200\n",
      "89/89 [==============================] - 8s 87ms/step - loss: 2.9709 - accuracy: 0.2999\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 8s 88ms/step - loss: 2.9620 - accuracy: 0.3021\n",
      "Epoch 82/200\n",
      "89/89 [==============================] - 8s 94ms/step - loss: 2.8739 - accuracy: 0.3186\n",
      "Epoch 83/200\n",
      "89/89 [==============================] - 8s 91ms/step - loss: 2.7994 - accuracy: 0.3305\n",
      "Epoch 84/200\n",
      "89/89 [==============================] - 8s 94ms/step - loss: 2.7399 - accuracy: 0.3418\n",
      "Epoch 85/200\n",
      "89/89 [==============================] - 10s 108ms/step - loss: 2.6798 - accuracy: 0.3523\n",
      "Epoch 86/200\n",
      "89/89 [==============================] - 10s 111ms/step - loss: 2.6230 - accuracy: 0.3624\n",
      "Epoch 87/200\n",
      "89/89 [==============================] - 9s 102ms/step - loss: 2.5655 - accuracy: 0.3799\n",
      "Epoch 88/200\n",
      "89/89 [==============================] - 8s 93ms/step - loss: 2.4736 - accuracy: 0.3923\n",
      "Epoch 89/200\n",
      "89/89 [==============================] - 9s 100ms/step - loss: 2.4205 - accuracy: 0.4062\n",
      "Epoch 90/200\n",
      "89/89 [==============================] - 10s 109ms/step - loss: 2.3666 - accuracy: 0.4188\n",
      "Epoch 91/200\n",
      "89/89 [==============================] - 9s 98ms/step - loss: 2.3919 - accuracy: 0.4140\n",
      "Epoch 92/200\n",
      "89/89 [==============================] - 9s 97ms/step - loss: 2.3362 - accuracy: 0.4238\n",
      "Epoch 93/200\n",
      "89/89 [==============================] - 9s 98ms/step - loss: 2.2927 - accuracy: 0.4338\n",
      "Epoch 94/200\n",
      "89/89 [==============================] - 8s 95ms/step - loss: 2.2491 - accuracy: 0.4426\n",
      "Epoch 95/200\n",
      "89/89 [==============================] - 8s 91ms/step - loss: 2.1718 - accuracy: 0.4609\n",
      "Epoch 96/200\n",
      "89/89 [==============================] - 9s 103ms/step - loss: 2.1265 - accuracy: 0.4711\n",
      "Epoch 97/200\n",
      "89/89 [==============================] - 9s 103ms/step - loss: 2.0770 - accuracy: 0.4821\n",
      "Epoch 98/200\n",
      "89/89 [==============================] - 9s 104ms/step - loss: 2.0350 - accuracy: 0.4869\n",
      "Epoch 99/200\n",
      "89/89 [==============================] - 10s 109ms/step - loss: 2.0172 - accuracy: 0.4922\n",
      "Epoch 100/200\n",
      "89/89 [==============================] - 9s 106ms/step - loss: 2.0289 - accuracy: 0.4853\n",
      "Epoch 101/200\n",
      "89/89 [==============================] - 12s 138ms/step - loss: 1.9554 - accuracy: 0.5061\n",
      "Epoch 102/200\n",
      "89/89 [==============================] - 13s 143ms/step - loss: 1.8764 - accuracy: 0.5271\n",
      "Epoch 103/200\n",
      "89/89 [==============================] - 10s 107ms/step - loss: 1.8342 - accuracy: 0.5350\n",
      "Epoch 104/200\n",
      "89/89 [==============================] - 11s 122ms/step - loss: 1.7928 - accuracy: 0.5447\n",
      "Epoch 105/200\n",
      "89/89 [==============================] - 10s 110ms/step - loss: 1.7529 - accuracy: 0.5520\n",
      "Epoch 106/200\n",
      "89/89 [==============================] - 9s 104ms/step - loss: 1.7191 - accuracy: 0.5625\n",
      "Epoch 107/200\n",
      "89/89 [==============================] - 9s 105ms/step - loss: 1.6865 - accuracy: 0.5690\n",
      "Epoch 108/200\n",
      "89/89 [==============================] - 9s 106ms/step - loss: 1.6561 - accuracy: 0.5764\n",
      "Epoch 109/200\n",
      "89/89 [==============================] - 9s 106ms/step - loss: 1.6399 - accuracy: 0.5812\n",
      "Epoch 110/200\n",
      "89/89 [==============================] - 9s 104ms/step - loss: 1.7789 - accuracy: 0.5570\n",
      "Epoch 111/200\n",
      "89/89 [==============================] - 9s 104ms/step - loss: 1.8314 - accuracy: 0.5393\n",
      "Epoch 112/200\n",
      "89/89 [==============================] - 9s 104ms/step - loss: 1.7212 - accuracy: 0.5643\n",
      "Epoch 113/200\n",
      "89/89 [==============================] - 10s 108ms/step - loss: 1.6571 - accuracy: 0.5782\n",
      "Epoch 114/200\n",
      "89/89 [==============================] - 9s 106ms/step - loss: 1.6162 - accuracy: 0.5910\n",
      "Epoch 115/200\n",
      "89/89 [==============================] - 9s 105ms/step - loss: 1.6422 - accuracy: 0.5863\n",
      "Epoch 116/200\n",
      "89/89 [==============================] - 9s 106ms/step - loss: 1.6201 - accuracy: 0.5892\n",
      "Epoch 117/200\n",
      "89/89 [==============================] - 9s 106ms/step - loss: 1.7444 - accuracy: 0.5781\n",
      "Epoch 118/200\n",
      "89/89 [==============================] - 9s 104ms/step - loss: 1.9795 - accuracy: 0.5225\n",
      "Epoch 119/200\n",
      "89/89 [==============================] - 9s 103ms/step - loss: 1.8324 - accuracy: 0.5428\n",
      "Epoch 120/200\n",
      "89/89 [==============================] - 9s 105ms/step - loss: 1.7479 - accuracy: 0.5615\n",
      "Epoch 121/200\n",
      "89/89 [==============================] - 10s 107ms/step - loss: 1.6506 - accuracy: 0.5899\n",
      "Epoch 122/200\n",
      "89/89 [==============================] - 9s 105ms/step - loss: 1.5980 - accuracy: 0.5959\n",
      "Epoch 123/200\n",
      "89/89 [==============================] - 9s 106ms/step - loss: 1.5632 - accuracy: 0.6106\n",
      "Epoch 124/200\n",
      "89/89 [==============================] - 9s 104ms/step - loss: 1.6653 - accuracy: 0.5903\n",
      "Epoch 125/200\n",
      "89/89 [==============================] - 9s 105ms/step - loss: 1.5417 - accuracy: 0.6079\n",
      "Epoch 126/200\n",
      "89/89 [==============================] - 9s 106ms/step - loss: 1.8909 - accuracy: 0.5390\n",
      "Epoch 127/200\n",
      "89/89 [==============================] - 9s 104ms/step - loss: 1.7798 - accuracy: 0.5569\n",
      "Epoch 128/200\n",
      "89/89 [==============================] - 9s 105ms/step - loss: 1.6787 - accuracy: 0.5810\n",
      "Epoch 129/200\n",
      "89/89 [==============================] - 9s 104ms/step - loss: 1.7104 - accuracy: 0.5806\n",
      "Epoch 130/200\n",
      "89/89 [==============================] - 8s 93ms/step - loss: 1.7558 - accuracy: 0.5630\n",
      "Epoch 131/200\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 1.6959 - accuracy: 0.5771\n",
      "Epoch 132/200\n",
      "89/89 [==============================] - 7s 84ms/step - loss: 1.6305 - accuracy: 0.5993\n",
      "Epoch 133/200\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 1.6327 - accuracy: 0.5974\n",
      "Epoch 134/200\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 1.6043 - accuracy: 0.6048\n",
      "Epoch 135/200\n",
      "89/89 [==============================] - 8s 88ms/step - loss: 1.5662 - accuracy: 0.6093\n",
      "Epoch 136/200\n",
      "89/89 [==============================] - 8s 85ms/step - loss: 1.5786 - accuracy: 0.6081\n",
      "Epoch 137/200\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 1.5348 - accuracy: 0.6160\n",
      "Epoch 138/200\n",
      "89/89 [==============================] - 8s 85ms/step - loss: 1.4724 - accuracy: 0.6285\n",
      "Epoch 139/200\n",
      "89/89 [==============================] - 7s 84ms/step - loss: 1.4367 - accuracy: 0.6417\n",
      "Epoch 140/200\n",
      "89/89 [==============================] - 8s 84ms/step - loss: 1.3826 - accuracy: 0.6579\n",
      "Epoch 141/200\n",
      "89/89 [==============================] - 8s 85ms/step - loss: 1.3500 - accuracy: 0.6620\n",
      "Epoch 142/200\n",
      "89/89 [==============================] - 7s 84ms/step - loss: 1.3144 - accuracy: 0.6721\n",
      "Epoch 143/200\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 1.2934 - accuracy: 0.6802\n",
      "Epoch 144/200\n",
      "89/89 [==============================] - 7s 84ms/step - loss: 1.2882 - accuracy: 0.6787\n",
      "Epoch 145/200\n",
      "89/89 [==============================] - 8s 87ms/step - loss: 1.2470 - accuracy: 0.6871\n",
      "Epoch 146/200\n",
      "89/89 [==============================] - 8s 91ms/step - loss: 1.2283 - accuracy: 0.6934\n",
      "Epoch 147/200\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 1.1988 - accuracy: 0.7033\n",
      "Epoch 148/200\n",
      "89/89 [==============================] - 7s 78ms/step - loss: 1.1988 - accuracy: 0.7033\n",
      "Epoch 149/200\n",
      "89/89 [==============================] - 8s 85ms/step - loss: 1.1947 - accuracy: 0.7046\n",
      "Epoch 150/200\n",
      "89/89 [==============================] - 9s 100ms/step - loss: 1.1586 - accuracy: 0.7151\n",
      "Epoch 151/200\n",
      "89/89 [==============================] - 8s 92ms/step - loss: 1.1711 - accuracy: 0.7073\n",
      "Epoch 152/200\n",
      "89/89 [==============================] - 8s 93ms/step - loss: 1.1812 - accuracy: 0.6937\n",
      "Epoch 153/200\n",
      "89/89 [==============================] - 8s 89ms/step - loss: 1.0975 - accuracy: 0.7190\n",
      "Epoch 154/200\n",
      "89/89 [==============================] - 8s 89ms/step - loss: 1.0334 - accuracy: 0.7399\n",
      "Epoch 155/200\n",
      "89/89 [==============================] - 8s 89ms/step - loss: 0.9990 - accuracy: 0.7503\n",
      "Epoch 156/200\n",
      "89/89 [==============================] - 8s 88ms/step - loss: 0.9901 - accuracy: 0.7547\n",
      "Epoch 157/200\n",
      "89/89 [==============================] - 8s 88ms/step - loss: 0.9693 - accuracy: 0.7580\n",
      "Epoch 158/200\n",
      "89/89 [==============================] - 8s 89ms/step - loss: 0.9234 - accuracy: 0.7708\n",
      "Epoch 159/200\n",
      "89/89 [==============================] - 8s 87ms/step - loss: 0.8884 - accuracy: 0.7784\n",
      "Epoch 160/200\n",
      "89/89 [==============================] - 8s 88ms/step - loss: 0.8598 - accuracy: 0.7876\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 8s 89ms/step - loss: 0.8255 - accuracy: 0.7994\n",
      "Epoch 162/200\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 0.8107 - accuracy: 0.8052\n",
      "Epoch 163/200\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 0.7778 - accuracy: 0.8136\n",
      "Epoch 164/200\n",
      "89/89 [==============================] - 8s 87ms/step - loss: 0.7570 - accuracy: 0.8176\n",
      "Epoch 165/200\n",
      "89/89 [==============================] - 8s 90ms/step - loss: 0.7599 - accuracy: 0.8205\n",
      "Epoch 166/200\n",
      "89/89 [==============================] - 8s 89ms/step - loss: 0.7285 - accuracy: 0.8283\n",
      "Epoch 167/200\n",
      "89/89 [==============================] - 8s 92ms/step - loss: 0.7118 - accuracy: 0.8354\n",
      "Epoch 168/200\n",
      "89/89 [==============================] - 8s 92ms/step - loss: 0.7226 - accuracy: 0.8282\n",
      "Epoch 169/200\n",
      "89/89 [==============================] - 10s 112ms/step - loss: 0.6915 - accuracy: 0.8397\n",
      "Epoch 170/200\n",
      "89/89 [==============================] - 9s 106ms/step - loss: 0.6655 - accuracy: 0.8474\n",
      "Epoch 171/200\n",
      "89/89 [==============================] - 10s 107ms/step - loss: 0.7160 - accuracy: 0.8335\n",
      "Epoch 172/200\n",
      "89/89 [==============================] - 10s 107ms/step - loss: 0.6994 - accuracy: 0.8402\n",
      "Epoch 173/200\n",
      "89/89 [==============================] - 10s 108ms/step - loss: 0.7018 - accuracy: 0.8368\n",
      "Epoch 174/200\n",
      "89/89 [==============================] - 10s 109ms/step - loss: 0.7088 - accuracy: 0.8340\n",
      "Epoch 175/200\n",
      "89/89 [==============================] - 10s 110ms/step - loss: 0.6699 - accuracy: 0.8461\n",
      "Epoch 176/200\n",
      "89/89 [==============================] - 10s 108ms/step - loss: 0.6308 - accuracy: 0.8571\n",
      "Epoch 177/200\n",
      "89/89 [==============================] - 10s 107ms/step - loss: 0.5961 - accuracy: 0.8686\n",
      "Epoch 178/200\n",
      "89/89 [==============================] - 10s 109ms/step - loss: 0.5708 - accuracy: 0.8738\n",
      "Epoch 179/200\n",
      "89/89 [==============================] - 10s 110ms/step - loss: 0.5548 - accuracy: 0.8784\n",
      "Epoch 180/200\n",
      "89/89 [==============================] - 10s 108ms/step - loss: 0.6200 - accuracy: 0.8637\n",
      "Epoch 181/200\n",
      "89/89 [==============================] - 10s 109ms/step - loss: 0.7078 - accuracy: 0.8449\n",
      "Epoch 182/200\n",
      "89/89 [==============================] - 10s 108ms/step - loss: 0.7657 - accuracy: 0.8235\n",
      "Epoch 183/200\n",
      "89/89 [==============================] - 10s 109ms/step - loss: 0.7358 - accuracy: 0.8290\n",
      "Epoch 184/200\n",
      "89/89 [==============================] - 10s 109ms/step - loss: 0.7089 - accuracy: 0.8430\n",
      "Epoch 185/200\n",
      "89/89 [==============================] - 10s 111ms/step - loss: 0.6328 - accuracy: 0.8599\n",
      "Epoch 186/200\n",
      "89/89 [==============================] - 10s 108ms/step - loss: 0.5893 - accuracy: 0.8723\n",
      "Epoch 187/200\n",
      "89/89 [==============================] - 10s 110ms/step - loss: 0.5464 - accuracy: 0.8856\n",
      "Epoch 188/200\n",
      "89/89 [==============================] - 10s 109ms/step - loss: 0.5258 - accuracy: 0.8891\n",
      "Epoch 189/200\n",
      "89/89 [==============================] - 10s 117ms/step - loss: 0.5251 - accuracy: 0.8870\n",
      "Epoch 190/200\n",
      "89/89 [==============================] - 10s 117ms/step - loss: 0.4999 - accuracy: 0.8941\n",
      "Epoch 191/200\n",
      "89/89 [==============================] - 11s 126ms/step - loss: 0.4968 - accuracy: 0.8934\n",
      "Epoch 192/200\n",
      "89/89 [==============================] - 13s 148ms/step - loss: 0.6831 - accuracy: 0.8628\n",
      "Epoch 193/200\n",
      "89/89 [==============================] - 10s 112ms/step - loss: 1.6022 - accuracy: 0.6972\n",
      "Epoch 194/200\n",
      "89/89 [==============================] - 9s 103ms/step - loss: 2.2877 - accuracy: 0.5368\n",
      "Epoch 195/200\n",
      "89/89 [==============================] - 10s 114ms/step - loss: 2.0926 - accuracy: 0.5279\n",
      "Epoch 196/200\n",
      "89/89 [==============================] - 11s 121ms/step - loss: 1.1493 - accuracy: 0.7184\n",
      "Epoch 197/200\n",
      "89/89 [==============================] - 11s 119ms/step - loss: 0.9963 - accuracy: 0.7636\n",
      "Epoch 198/200\n",
      "89/89 [==============================] - 11s 119ms/step - loss: 0.8914 - accuracy: 0.7932\n",
      "Epoch 199/200\n",
      "89/89 [==============================] - 10s 116ms/step - loss: 0.9179 - accuracy: 0.7888\n",
      "Epoch 200/200\n",
      "89/89 [==============================] - 10s 117ms/step - loss: 0.8240 - accuracy: 0.8086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x236806fba90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,batch_size=128,epochs=200,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d08490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_mobydick_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37a7df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer,open('my_simpletokenizer','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac991b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "992c2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    model : model that was trained on text data\n",
    "    tokenizer : tokenizer that was fit on text data\n",
    "    seq_len : length of training sequence\n",
    "    seed_text : raw string text to serve as the seed\n",
    "    num_gen_words : number of words to be generated by model\n",
    "    '''\n",
    "    \n",
    "    # Final Output\n",
    "    output_text = []\n",
    "    \n",
    "    # Intial Seed Sequence\n",
    "    input_text = seed_text\n",
    "    \n",
    "    # Create num_gen_words\n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        # Take the input text string and encode it to a sequence\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        # Pad sequences to our trained rate (50 words in the video)\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        \n",
    "        # Predict Class Probabilities for each word\n",
    "        pred_word_ind = model.predict(pad_encoded, verbose=0)\n",
    "        pred_word_=np.argmax(pred_word_ind,axis=1)[0]\n",
    "        \n",
    "        # Grab word\n",
    "        pred_word = tokenizer.index_word[pred_word_] \n",
    "        \n",
    "        # Update the sequence of input text (shifting one over with the new word)\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    # Make it look like a sentence.\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "555c6003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'me',\n",
       " 'ishmael',\n",
       " 'some',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'how',\n",
       " 'long',\n",
       " 'precisely',\n",
       " 'having',\n",
       " 'little',\n",
       " 'or',\n",
       " 'no',\n",
       " 'money',\n",
       " 'in',\n",
       " 'my',\n",
       " 'purse',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'to',\n",
       " 'interest',\n",
       " 'me',\n",
       " 'on']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e54d4e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(101)\n",
    "random_pick= random.randint(0,len(text_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0ae4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed_text = text_sequences[random_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6aa1e4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thought i to myself the man 's a human being just as i am he has just as much reason to fear me as i have\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text = ' '.join(random_seed_text)\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "895330ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to be afraid of him better could do that comfortable that since all down into the idol yet took last far was n't coffin four low stooping or woollen as the case from a dark complexioned half ' ' not not rather a chief bagging a cape put it unmethodically\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11fa3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading a model trained with 500 epochs\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f32748ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_mobydick_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = load(open('epochBIG','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b236dd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to be afraid of him better could do that comfortable that since all down into the idol yet took last far was n't coffin four\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918e969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
